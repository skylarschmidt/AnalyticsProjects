{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc63b31e",
   "metadata": {},
   "source": [
    "## Import 'vehicles' Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea2f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6479ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\skyla\\Downloads\\vehicles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77a0eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'url', 'region', 'region_url', 'price', 'year', 'manufacturer',\n",
       "       'model', 'condition', 'cylinders', 'fuel', 'odometer', 'title_status',\n",
       "       'transmission', 'VIN', 'drive', 'size', 'type', 'paint_color',\n",
       "       'image_url', 'description', 'county', 'state', 'lat', 'long',\n",
       "       'posting_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807e28f",
   "metadata": {},
   "source": [
    "## Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab701e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL doesn't add anything to our dataset\n",
    "df = df.drop('url', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39cd720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Region URL doesn't add anything to our dataset\n",
    "df = df.drop('region_url', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b64299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image URL doesn't add anything to our dataset\n",
    "df = df.drop('image_url', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0698dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a useful column but slows down the dataset too much. \n",
    "#I don't have the hardware or time to realistically transform and analyse this column.\n",
    "df = df.drop('description', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab688f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latitude is out of scope for analysis \n",
    "df = df.drop('lat', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "916b01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Longitude is out of scope for analysis \n",
    "df = df.drop('long', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b019a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Too many null values and no way to impute\n",
    "df = df.drop('paint_color', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0b88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'price' is NaN or 'price' equals 0\n",
    "df = df.dropna(subset=['price'])\n",
    "df = df[df['price'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc03cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold for columns to keep (at least 70% data present)\n",
    "threshold_columns = len(df) * 0.7\n",
    "\n",
    "# Remove columns with more than 30% missing values\n",
    "df = df.dropna(thresh=threshold_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7e7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold for rows to keep (at least 70% data present)\n",
    "threshold_rows = len(df.columns) * 0.7\n",
    "\n",
    "# Remove rows with more than 30% missing values\n",
    "df = df.dropna(thresh=threshold_rows, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14dbcc",
   "metadata": {},
   "source": [
    "## Impute Date Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e40454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Posting Date to DateTime format\n",
    "df['posting_date'] = pd.to_datetime(df['posting_date'], utc=True, format='%Y-%m-%dT%H:%M:%S%z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88de7d6",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a9de73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393443, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dad3490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'region', 'price', 'year', 'manufacturer', 'model', 'fuel',\n",
       "       'odometer', 'title_status', 'transmission', 'type', 'state',\n",
       "       'posting_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9edf16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns that have been removed are \n",
    "#url\n",
    "#region_url\n",
    "#condition\n",
    "#cylinders\n",
    "#VIN\n",
    "#drive\n",
    "#size\n",
    "#image_url\n",
    "#county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "229a7e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            int64\n",
       "region                       object\n",
       "price                         int64\n",
       "year                        float64\n",
       "manufacturer                 object\n",
       "model                        object\n",
       "fuel                         object\n",
       "odometer                    float64\n",
       "title_status                 object\n",
       "transmission                 object\n",
       "type                         object\n",
       "state                        object\n",
       "posting_date    datetime64[ns, UTC]\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39fc7b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27   2021-05-04 17:31:18+00:00\n",
      "28   2021-05-04 17:31:08+00:00\n",
      "29   2021-05-04 17:31:25+00:00\n",
      "30   2021-05-04 15:41:31+00:00\n",
      "31   2021-05-03 19:02:03+00:00\n",
      "Name: posting_date, dtype: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the 'posting_date' to verify the conversion\n",
    "print(df['posting_date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3568bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date features\n",
    "df['posting_year'] = df['posting_date'].dt.year\n",
    "df['posting_month'] = df['posting_date'].dt.month\n",
    "df['posting_day'] = df['posting_date'].dt.day\n",
    "df['posting_weekday'] = df['posting_date'].dt.weekday  # Monday=0, Sunday=6\n",
    "df['posting_hour'] = df['posting_date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7a81cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                posting_date  posting_year  posting_month  posting_day  \\\n",
      "27 2021-05-04 17:31:18+00:00          2021              5            4   \n",
      "28 2021-05-04 17:31:08+00:00          2021              5            4   \n",
      "29 2021-05-04 17:31:25+00:00          2021              5            4   \n",
      "30 2021-05-04 15:41:31+00:00          2021              5            4   \n",
      "31 2021-05-03 19:02:03+00:00          2021              5            3   \n",
      "\n",
      "    posting_weekday  posting_hour  \n",
      "27                1            17  \n",
      "28                1            17  \n",
      "29                1            17  \n",
      "30                1            15  \n",
      "31                0            19  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame to inspect the new date-related columns\n",
    "print(df[['posting_date', 'posting_year', 'posting_month', 'posting_day', 'posting_weekday', 'posting_hour']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0416ed",
   "metadata": {},
   "source": [
    "## Impute Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b14a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to fill in missing 'Manufacturer' values using 'Model' values, identical model values\n",
    "#That are matching, are used to fill in 'Manufacturer' using the Mean 'Model' value that matches\n",
    "# Create a dictionary to store mapping of models to most frequent manufacturers\n",
    "model_to_most_frequent_manufacturer = {}\n",
    "\n",
    "# Populate the dictionary with the most frequent 'manufacturer' values for each 'model'\n",
    "for model, group in df.groupby('model'):\n",
    "    if not group['manufacturer'].isnull().all():\n",
    "        most_frequent_manufacturer = group['manufacturer'].mode().iloc[0]\n",
    "        model_to_most_frequent_manufacturer[model] = most_frequent_manufacturer\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the 'manufacturer' value is empty for the current row\n",
    "    if pd.isnull(row['manufacturer']):\n",
    "        # Get the 'model' value for the current row\n",
    "        model = row['model']\n",
    "        # If the 'model' is in the dictionary, fill in the missing 'manufacturer' value\n",
    "        if model in model_to_most_frequent_manufacturer:\n",
    "            df.at[index, 'manufacturer'] = model_to_most_frequent_manufacturer[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4f901b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27    sierra 1500 crew cab slt\n",
      "28              silverado 1500\n",
      "29         silverado 1500 crew\n",
      "30        tundra double cab sr\n",
      "31                   f-150 xlt\n",
      "Name: model, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['model'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "602d603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       model base_model\n",
      "27  sierra 1500 crew cab slt     sierra\n",
      "28            silverado 1500  silverado\n",
      "29       silverado 1500 crew  silverado\n",
      "30      tundra double cab sr     tundra\n",
      "31                 f-150 xlt      f-150\n"
     ]
    }
   ],
   "source": [
    "def extract_base_model(model):\n",
    "    # Check if the model is a string; if not, convert it to string\n",
    "    # This also handles NaN and None values by converting them to the string 'nan'\n",
    "    base_model = str(model).split()[0]\n",
    "    return base_model\n",
    "\n",
    "# Apply the function to the 'model' column to create a new 'base_model' column\n",
    "df['base_model'] = df['model'].apply(extract_base_model)\n",
    "\n",
    "# Display the result to verify\n",
    "print(df[['model', 'base_model']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5115d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of purely numeric 'base_model' entries: 26411\n"
     ]
    }
   ],
   "source": [
    "# Count the rows where 'base_model' is purely numeric\n",
    "numeric_base_models_count = df[df['base_model'].str.isdigit()].shape[0]\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of purely numeric 'base_model' entries: {numeric_base_models_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6649951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393443, 19)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fa48a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367032, 19)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where 'base_model' is purely numeric\n",
    "df = df[~df['base_model'].str.isdigit()]\n",
    "\n",
    "# Display the result or check the shape to confirm removal\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8528ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 'base_model' values: 4690\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique 'base_model' values in the filtered DataFrame\n",
    "unique_base_models_count = df['base_model'].nunique()\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of unique 'base_model' values: {unique_base_models_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ef729ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing this with Base Model\n",
    "df = df.drop('model', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2459ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now have the aggregated values and can remove this to simply our dataset\n",
    "df = df.drop('posting_date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b71c1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_base_models = df['base_model'].unique()\n",
    "unique_base_models_df = pd.DataFrame(unique_base_models, columns=['base_model'])\n",
    "unique_base_models_df.to_csv('unique_base_models.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b500447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of some of the unreadable values\n",
    "# List of strings to remove from base_model values\n",
    "strings_to_remove = [\n",
    "    \"-\", \"%\", \"-2018\", \"2-Jan\", \"3-Sep\", \"4-Mar\", \"155,037\", \"7.3\", \"3.2\", \"1.5\",\n",
    "    \"198.5\", \"3.5\", \"(cng)\", \"*matrix*\", \"*\", \"2.5\", \"4.6\", \"-350\", \"99.5\", \"2003\",\n",
    "    \"-210\", \"-300\", \"135,826\", \"t\", \"(s)port\", \"2.4\", \"-150\", \"1986\", \"$362.47\", \"&\", \"2\",\n",
    "    \"#NAME?\", \":\", \"/\", \"//\", \"45538\", \"45293\", '.', '2003', '1986', '2', '9/3/2024', '1/2/2024'\n",
    "]\n",
    "\n",
    "# Define the list of IDs to delete base_model values for\n",
    "ids_to_delete = [7302049019, 7303726176, 7303889514, 7304172920, 7304766070, 7306374142,\n",
    "7307179181, 7307984950, 7308311655, 7308436673, 7310003852, 7310724097,\n",
    "7310905114, 7311156124, 7312639238, 7312807312, 7313478534, 7313615145,\n",
    "7314081937, 7314348336, 7314500907, 7315012429, 7315076544, 7315165698,\n",
    "7315229169, 7315376783, 7315377882, 7315378199, 7315379051, 7315514859,\n",
    "7316150205, 7316150889, 7316181280, 7316181424, 7316450659, 7316525434,\n",
    "7316572113, 7316803175]\n",
    "\n",
    "\n",
    "\n",
    "# Remove base_model values containing specified strings\n",
    "df = df[~df['base_model'].isin(strings_to_remove)]\n",
    "\n",
    "# Remove rows with specific values from the DataFrame\n",
    "df = df[~df['manufacturer'].isin(['45538', '45293', '.', '2003', '1986', '2', '9/3/2024', '1/2/2024'])]\n",
    "\n",
    "# Delete the base_model values for the specified IDs\n",
    "df.loc[df['id'].isin(ids_to_delete), 'base_model'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7c4ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1 and Q3\n",
    "Q1 = df['price'].quantile(0.25)\n",
    "Q3 = df['price'].quantile(0.75)\n",
    "\n",
    "# Calculate the IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Adjust the multiplier for the upper bound to make it more lenient\n",
    "# and set a minimum price to remove unrealistically low prices\n",
    "multiplier = 5  # Adjusting this value can make the filter more lenient or strict\n",
    "lower_bound = Q1 - 1.5 * IQR  # You might keep the lower bound less changed\n",
    "upper_bound = Q3 + multiplier * IQR  # Increase the upper bound to be more lenient\n",
    "\n",
    "# Define a realistic minimum price to filter out $1 prices\n",
    "minimum_price = 100  # Example minimum price, adjust as needed\n",
    "\n",
    "# Remove outliers with adjusted criteria and save back to df\n",
    "df = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound) & (df['price'] > minimum_price)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acd23a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "# This line replaces any characters at the start of the string that are not letters or numbers with an empty string\n",
    "df['base_model'] = df['base_model'].str.replace('^[^a-zA-Z0-9]*', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f155f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify all unique, non-empty manufacturer names\n",
    "unique_manufacturers = df['manufacturer'].dropna().unique()\n",
    "\n",
    "# Step 2: Normalize the 'base_model' and manufacturer names for comparison\n",
    "df['normalized_base_model'] = df['base_model'].str.lower()\n",
    "\n",
    "# Step 3: For each unique manufacturer, try to find and fill missing manufacturer values based on 'base_model'\n",
    "for manufacturer in unique_manufacturers:\n",
    "    normalized_manufacturer = manufacturer.lower()\n",
    "    mask = df['manufacturer'].isna() & df['normalized_base_model'].str.contains(normalized_manufacturer)\n",
    "    df.loc[mask, 'manufacturer'] = manufacturer\n",
    "\n",
    "# Optional: Clean up by removing the temporary 'normalized_base_model' column\n",
    "df.drop('normalized_base_model', axis=1, inplace=True)\n",
    "\n",
    "# This code block goes through each row where 'manufacturer' is missing and checks if 'base_model' (ignoring case)\n",
    "# contains a string that matches one of the unique, non-missing manufacturer names. If a match is found,\n",
    "# it fills in the 'manufacturer' column with the corresponding manufacturer name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "707ef894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Odometer Upper Limit Outliars\n",
    "Q1 = df['odometer'].quantile(0.25)\n",
    "Q3 = df['odometer'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "df = df[df['odometer'] <= upper_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1af65347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Year Lower Limit Outliers\n",
    "Q1_year = df['year'].quantile(0.25)\n",
    "Q3_year = df['year'].quantile(0.75)\n",
    "IQR_year = Q3_year - Q1_year\n",
    "lower_limit_year = Q1_year - 1.5 * IQR_year\n",
    "\n",
    "df = df[df['year'] >= lower_limit_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aafe378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'manufacturer' values after filling: 8714\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from base_model to manufacturer based on non-null entries in the dataframe\n",
    "valid_manufacturer_base_model = df.dropna(subset=['manufacturer', 'base_model'])\n",
    "base_model_to_manufacturer = valid_manufacturer_base_model.drop_duplicates(subset=['base_model'])[['base_model', 'manufacturer']].set_index('base_model')['manufacturer'].to_dict()\n",
    "\n",
    "# Function to apply the mapping to fill missing manufacturer values\n",
    "def fill_manufacturer(row):\n",
    "    if pd.isnull(row['manufacturer']) and row['base_model'] in base_model_to_manufacturer:\n",
    "        return base_model_to_manufacturer[row['base_model']]\n",
    "    else:\n",
    "        return row['manufacturer']\n",
    "\n",
    "# Apply the function to fill missing 'manufacturer' values based on 'base_model'\n",
    "df['manufacturer'] = df.apply(fill_manufacturer, axis=1)\n",
    "\n",
    "# Optionally, to check how many 'manufacturer' values are still missing after this operation\n",
    "missing_manufacturer_after = df['manufacturer'].isnull().sum()\n",
    "print(f\"Missing 'manufacturer' values after filling: {missing_manufacturer_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b60f5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343615, 17)\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows where 'base_model' is null\n",
    "df = df.dropna(subset=['base_model'])\n",
    "\n",
    "# Optionally, to check the new shape of the dataframe after removal\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30fb32a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     0\n",
      "region                 0\n",
      "price                  0\n",
      "year                   0\n",
      "manufacturer        8705\n",
      "fuel                1887\n",
      "odometer               0\n",
      "title_status        6513\n",
      "transmission        1385\n",
      "type               70898\n",
      "state                  0\n",
      "posting_year           0\n",
      "posting_month          0\n",
      "posting_day            0\n",
      "posting_weekday        0\n",
      "posting_hour           0\n",
      "base_model             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count null values in each column\n",
    "null_values_count = df.isnull().sum()\n",
    "\n",
    "print(null_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85b409e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8547"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a mapping of known base models or model prefixes to manufacturers\n",
    "model_prefix_to_manufacturer = {\n",
    "    'sierra': 'gmc',\n",
    "    'silverado': 'chevrolet',\n",
    "    'tundra': 'toyota',\n",
    "    'f-150': 'ford',\n",
    "    'f150': 'ford',\n",
    "    'tacoma': 'toyota',\n",
    "    'corvette': 'chevrolet',\n",
    "    'wrangler': 'jeep',\n",
    "    'camaro': 'chevrolet',\n",
    "    'ranger': 'ford',\n",
    "    'frontier': 'nissan',\n",
    "    'mx-5': 'mazda',\n",
    "    'xt4': 'cadillac',\n",
    "    'f250': 'ford',\n",
    "    'renegade': 'jeep',\n",
    "    'odyssey': 'honda',\n",
    "    'mustang': 'ford',\n",
    "    'f450': 'ford',\n",
    "    'charger': 'dodge',\n",
    "    # Continue adding more mappings as needed based on the dataset and common knowledge\n",
    "}\n",
    "\n",
    "# Function to infer manufacturer from base_model using the mapping\n",
    "def infer_manufacturer_from_base_model(base_model):\n",
    "    base_model = base_model.lower()  # Ensure matching is case-insensitive\n",
    "    for model_prefix, manufacturer in model_prefix_to_manufacturer.items():\n",
    "        if model_prefix in base_model:\n",
    "            return manufacturer\n",
    "    return None\n",
    "\n",
    "# Apply the function to fill missing 'manufacturer' values based on 'base_model'\n",
    "df['manufacturer'] = df.apply(\n",
    "    lambda row: infer_manufacturer_from_base_model(row['base_model']) if pd.isnull(row['manufacturer']) else row['manufacturer'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Verify the application by checking the number of missing 'manufacturer' values after inference\n",
    "missing_manufacturer_after_inference = df['manufacturer'].isnull().sum()\n",
    "\n",
    "missing_manufacturer_after_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7151e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sierra': 'chevrolet', 'silverado': 'chevrolet', 'tundra': 'toyota', 'f-150': 'ford', 'tacoma': 'toyota', 'colorado': 'chevrolet', 'corvette': 'chevrolet', 'wrangler': 'jeep', 'camaro': 'chevrolet', 'ranger': 'jeep'}\n"
     ]
    }
   ],
   "source": [
    "# Known manufacturers list for matching\n",
    "known_manufacturers = known_manufacturers = [\n",
    "    'ford', 'chevrolet', 'toyota', 'honda', 'nissan', 'jeep', 'gmc', 'dodge', \n",
    "    'bmw', 'mercedes', 'audi', 'lexus', 'volkswagen', 'subaru', 'hyundai', \n",
    "    'kia', 'mazda', 'porsche', 'ferrari', 'lamborghini', 'tesla', 'volvo', \n",
    "    'mitsubishi', 'land rover', 'jaguar', 'buick', 'cadillac', 'chrysler', \n",
    "    'lincoln', 'infiniti', 'acura', 'alfa romeo', 'fiat', 'genesis', 'mini', \n",
    "    'suzuki', 'saab', 'scion', 'hummer', 'maserati', 'isuzu', 'hino', 'bentley', \n",
    "    'mack', 'smart', 'lotus'\n",
    "]\n",
    "\n",
    "\n",
    "# Function to infer manufacturer from base_model\n",
    "def infer_manufacturer_from_base_model(base_model):\n",
    "    if pd.isna(base_model) or not isinstance(base_model, str):\n",
    "        return None\n",
    "    base_model = base_model.lower()\n",
    "    for manufacturer in known_manufacturers:\n",
    "        if manufacturer in base_model:\n",
    "            return manufacturer.capitalize()\n",
    "    return None\n",
    "\n",
    "# Infer and fill missing 'manufacturer' based on 'base_model'\n",
    "df['manufacturer'] = df.apply(lambda row: infer_manufacturer_from_base_model(row['base_model']) if pd.isnull(row['manufacturer']) else row['manufacturer'], axis=1)\n",
    "\n",
    "# Extract the updated mapping of 'base_model' to 'manufacturer'\n",
    "updated_mapping = df[['base_model', 'manufacturer']].drop_duplicates().set_index('base_model')['manufacturer'].to_dict()\n",
    "\n",
    "# Print a portion of the updated mapping\n",
    "print({k: updated_mapping[k] for k in list(updated_mapping)[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eac9b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom mapping based on the list and additional knowledge\n",
    "custom_mapping = {\n",
    "    'International': 'International Trucks',\n",
    "    'Freightliner': 'Freightliner',\n",
    "    'oldsmobile': 'Oldsmobile',\n",
    "    'Sterling': 'Sterling Trucks',\n",
    "    'Kenworth': 'Kenworth',\n",
    "    'Workhorse': 'Workhorse',\n",
    "    'hyndai': 'Hyundai',\n",
    "    'chryler': 'Chrysler',\n",
    "    'caddilac': 'Cadillac',\n",
    "    'CHEVORLET': 'Chevrolet',\n",
    "    'Volkswagon': 'Volkswagen',\n",
    "    'Pierce': 'Pierce',\n",
    "    'PETERBILT': 'Peterbilt',\n",
    "    'ROLLS': 'Rolls-Royce',\n",
    "    'YAMAHA': 'Yamaha',\n",
    "    'Plymouth': 'Plymouth',\n",
    "    'Corvette': 'Chevrolet',  # Corvette is a model by Chevrolet\n",
    "    'Prius': 'Toyota',  # Prius is a model by Toyota\n",
    "    # Add more mappings based on the list and internet research\n",
    "}\n",
    "\n",
    "# Normalize the base_model column to match keys in custom_mapping (case-insensitive)\n",
    "df['base_model_normalized'] = df['base_model'].str.lower()\n",
    "\n",
    "# Function to infer manufacturer from base_model using the custom mapping\n",
    "def infer_manufacturer_custom(base_model_normalized, custom_mapping):\n",
    "    for model, manufacturer in custom_mapping.items():\n",
    "        if model.lower() == base_model_normalized:\n",
    "            return manufacturer\n",
    "    return None\n",
    "\n",
    "# Apply the function to fill missing 'manufacturer' values based on the custom mapping\n",
    "df['manufacturer'] = df.apply(\n",
    "    lambda row: infer_manufacturer_custom(row['base_model_normalized'], custom_mapping) if pd.isnull(row['manufacturer']) else row['manufacturer'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop the temporary normalized base_model column if no longer needed\n",
    "df.drop('base_model_normalized', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1160bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly define the custom mapping with new information from the updated list\n",
    "custom_mapping2 = {\n",
    "    'chysler': 'Chrysler',\n",
    "    'SILVERADO': 'Chevrolet',\n",
    "    'F250': 'Ford',\n",
    "    'F150': 'Ford',\n",
    "    'F-150': 'Ford',\n",
    "    'Grand': 'Jeep',  # Assuming it refers to Grand Cherokee\n",
    "    'POLARIS': 'Polaris',  # Manufacturer of ATVs and motorcycles, if relevant\n",
    "    'WOLF': None,  # This might need clarification\n",
    "    'Mercedez': 'Mercedes-Benz',  # Correcting typo\n",
    "    'Silverado': 'Chevrolet',\n",
    "    'VPG': 'VPG',  # Vehicle Production Group, known for MV-1\n",
    "    'hundai': 'Hyundai',  # Correcting typo\n",
    "    'Mustang': 'Ford',\n",
    "    'cheverolet': 'Chevrolet',  # Correcting typo\n",
    "    'Cheverolet': 'Chevrolet',\n",
    "    'Wrangler': 'Jeep',\n",
    "    'MB': 'Mercedes-Benz',\n",
    "    'MV-1': 'VPG',\n",
    "    'huyndai': 'Hyundai',  # Correcting typo\n",
    "    'Camaro': 'Chevrolet',\n",
    "    'F350': 'Ford',\n",
    "    'mercedez': 'Mercedes-Benz',  # Correcting typo\n",
    "    'Cadilac': 'Cadillac',  # Correcting typo\n",
    "    'volkwagen': 'Volkswagen',  # Correcting typo\n",
    "    'SL': 'Mercedes-Benz',  # Assuming it refers to the Mercedes SL-Class\n",
    "    # Add or update mappings as needed\n",
    "}\n",
    "\n",
    "# Normalize the base_model column to match keys in custom_mapping (case-insensitive)\n",
    "df['base_model_normalized'] = df['base_model'].str.lower()\n",
    "\n",
    "# Function to infer manufacturer from base_model using the custom mapping\n",
    "def infer_manufacturer_custom(base_model_normalized, custom_mapping2):\n",
    "    for model, manufacturer in custom_mapping2.items():\n",
    "        if model.lower() == base_model_normalized:\n",
    "            return manufacturer\n",
    "    return None\n",
    "\n",
    "# Apply the function to fill missing 'manufacturer' values based on the custom mapping\n",
    "df['manufacturer'] = df.apply(\n",
    "    lambda row: infer_manufacturer_custom(row['base_model_normalized'], custom_mapping2) if pd.isnull(row['manufacturer']) else row['manufacturer'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop the temporary normalized base_model column if no longer needed\n",
    "df.drop('base_model_normalized', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58ee425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the custom_mapping2 with new information\n",
    "custom_mapping3 = {\n",
    "    # Correcting typos and mapping models to manufacturers\n",
    "    'Gm': 'General Motors',\n",
    "    'suburu': 'Subaru',  # Typo correction\n",
    "    'Accord': 'Honda',  # Model to manufacturer\n",
    "    'CHEV.': 'Chevrolet',  # Abbreviation to full name\n",
    "    'CHEC': None,  # Unclear reference, needs clarification\n",
    "    'Keep': 'Jeep',  # Possibly a typo for Jeep\n",
    "    'PROMASTER': 'Ram',  # Model to manufacturer\n",
    "    'Nissa': 'Nissan',  # Typo correction\n",
    "    'chrystler': 'Chrysler',  # Typo correction\n",
    "    'chrylser': 'Chrysler',  # Typo correction\n",
    "    'PT': 'Chrysler',  # Referring to PT Cruiser\n",
    "    'F800': None,  # Likely a model of Ford trucks, needs clarification\n",
    "    'chevolet': 'Chevrolet',  # Typo correction\n",
    "    'RANGER': 'Ford',  # Model to manufacturer\n",
    "    'Yukon': 'GMC',  # Model to manufacturer\n",
    "    'sierra': 'GMC',  # Model to manufacturer\n",
    "    'mistubishi': 'Mitsubishi',  # Typo correction\n",
    "    'Hyundia': 'Hyundai',  # Typo correction\n",
    "    'Toyoya': 'Toyota',  # Typo correction\n",
    "    # General terms or unclear references are set to None or handled specifically if possible\n",
    "    'ALL': None,\n",
    "    'All': None,\n",
    "    'BUY': None,\n",
    "    'IC': None,  # Could refer to IC Bus, needs clarification\n",
    "    'Janesville': None,\n",
    "    'Flexible': None,\n",
    "    'Blue': None,\n",
    "    'to': None,\n",
    "    'SPECIAL': None,\n",
    "    'cars': None,\n",
    "    'Other': None,\n",
    "    'Mobility': None,\n",
    "    'Emergency': None,\n",
    "    'any': None,\n",
    "    'Any': None,\n",
    "    'NEW': None,\n",
    "    'blue': None,\n",
    "    'WOLF': None,\n",
    "    'Keystone': None,\n",
    "    'THE': None,\n",
    "    'Utilimaster': 'Utilimaster',  # Manufacturer of walk-in vans and commercial vehicles\n",
    "    'WATER': None,\n",
    "    'Club': None,\n",
    "    'CLICK': None,\n",
    "    'ASK': None,\n",
    "    'FABRIQUE': None,\n",
    "    'hiunday': 'Hyundai',\n",
    "    'SANTA': 'Hyundai',  # Likely referring to Hyundai Santa Fe\n",
    "    # Add more mappings or corrections as necessary\n",
    "}\n",
    "\n",
    "# Normalize the base_model column to match keys in custom_mapping (case-insensitive)\n",
    "df['base_model_normalized'] = df['base_model'].str.lower()\n",
    "\n",
    "# Function to infer manufacturer from base_model using the custom mapping\n",
    "def infer_manufacturer_custom(base_model_normalized, custom_mapping3):\n",
    "    for model, manufacturer in custom_mapping3.items():\n",
    "        if model.lower() == base_model_normalized:\n",
    "            return manufacturer\n",
    "    return None\n",
    "\n",
    "# Apply the function to fill missing 'manufacturer' values based on the custom mapping\n",
    "df['manufacturer'] = df.apply(\n",
    "    lambda row: infer_manufacturer_custom(row['base_model_normalized'], custom_mapping3) if pd.isnull(row['manufacturer']) else row['manufacturer'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop the temporary normalized base_model column if no longer needed\n",
    "df.drop('base_model_normalized', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "136c3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend custom_mapping2 with more specific mappings and corrections\n",
    "custom_mapping4 = {\n",
    "    # Correcting specific vehicle models to their manufacturers\n",
    "    'Tacoma': 'Toyota',\n",
    "    'Civic': 'Honda',\n",
    "    'F-350': 'Ford',\n",
    "    'Jetta': 'Volkswagen',\n",
    "    'Geo': 'Chevrolet',  # Geo was a brand under Chevrolet\n",
    "    'Eagle': None,  # Eagle was a brand under Chrysler, now defunct\n",
    "    'F-250': 'Ford',\n",
    "    'nissian': 'Nissan',  # Typo correction\n",
    "    'izusu': 'Isuzu',  # Typo correction\n",
    "    'Kawasaki': 'Kawasaki',  # Manufacturer of motorcycles and ATVs, if relevant\n",
    "    'BMX': None,  # Likely a typo for BMW, or referring to bicycles\n",
    "    'FRHT': None,  # Unclear reference, possibly meant for Freightliner\n",
    "    'SUPER': None,  # Too generic, possibly part of a model name\n",
    "    'bluebird': 'Blue Bird Corporation',  # Manufacturer of school and activity buses\n",
    "    'John': None,  # Could refer to John Deere, if agricultural vehicles are relevant\n",
    "    '2D8HN54159R611084': None,  # This appears to be a VIN, not applicable for manufacturer inference\n",
    "    # Handling generic or ambiguous terms by setting to None or applying specific logic if possible\n",
    "    'ALL': None,\n",
    "    'All': None,\n",
    "    'BUY': None,\n",
    "    'IC': 'IC Bus',  # Manufacturer of school buses and commercial buses\n",
    "    'Janesville': None,  # Likely refers to a location, not a manufacturer\n",
    "    'Flexible': None,\n",
    "    'Blue': None,\n",
    "    'to': None,\n",
    "    'SPECIAL': None,\n",
    "    'cars': None,\n",
    "    'Other': None,\n",
    "    'Mobility': None,\n",
    "    'Emergency': None,\n",
    "    'any': None,\n",
    "    'Any': None,\n",
    "    'NEW': None,\n",
    "    'blue': None,\n",
    "    'WOLF': None,\n",
    "    'Keystone': None,\n",
    "    'THE': None,\n",
    "    'WATER': None,\n",
    "    'Cars': None,\n",
    "    'CHEC': None,\n",
    "    'Club': None,\n",
    "    'F800': 'Ford',  # F800 is a model of Ford trucks\n",
    "    'Town': None,  # Possibly referring to Chrysler Town & Country\n",
    "    'wheelchair': None,\n",
    "    'todos': None,\n",
    "    'CLICK': None,\n",
    "    'Porshe': 'Porsche',\n",
    "    'am': None,  # Too generic, needs clarification\n",
    "    'Western': None,  # Could refer to Western Star Trucks if trucks are relevant\n",
    "    'ASK': None,\n",
    "    # Add more mappings or corrections as necessary\n",
    "}\n",
    "\n",
    "# Normalize the base_model column to match keys in custom_mapping (case-insensitive)\n",
    "df['base_model_normalized'] = df['base_model'].str.lower()\n",
    "\n",
    "# Function to infer manufacturer from base_model using the custom mapping\n",
    "def infer_manufacturer_custom(base_model_normalized, custom_mapping4):\n",
    "    for model, manufacturer in custom_mapping4.items():\n",
    "        if model.lower() == base_model_normalized:\n",
    "            return manufacturer\n",
    "    return None\n",
    "\n",
    "# Apply the function to fill missing 'manufacturer' values based on the custom mapping\n",
    "df['manufacturer'] = df.apply(\n",
    "    lambda row: infer_manufacturer_custom(row['base_model_normalized'], custom_mapping4) if pd.isnull(row['manufacturer']) else row['manufacturer'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop the temporary normalized base_model column if no longer needed\n",
    "df.drop('base_model_normalized', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b22f2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend custom_mapping2 with more specific mappings and corrections\n",
    "custom_mapping5 = {\n",
    "    # Additions based on identifiable models or brands\n",
    "    'Blue': 'Blue Bird Corporation',  # Assuming association with Blue Bird buses\n",
    "    'Thomas': 'Thomas Built Buses',\n",
    "    'Keystone': 'Keystone RV',\n",
    "    'Camry': 'Toyota',\n",
    "    'crysler': 'Chrysler',\n",
    "    'Chrsler': 'Chrysler',\n",
    "    'doge': 'Dodge',\n",
    "    'Infinti': 'Infiniti',\n",
    "    # Other entries are too generic or need clarification; handle as appropriate\n",
    "}\n",
    "\n",
    "\n",
    "# Normalize the base_model column to match keys in custom_mapping (case-insensitive)\n",
    "df['base_model_normalized'] = df['base_model'].str.lower()\n",
    "\n",
    "# Function to infer manufacturer from base_model using the custom mapping\n",
    "def infer_manufacturer_custom(base_model_normalized, custom_mapping5):\n",
    "    for model, manufacturer in custom_mapping5.items():\n",
    "        if model.lower() == base_model_normalized:\n",
    "            return manufacturer\n",
    "    return None\n",
    "\n",
    "# Apply the function to fill missing 'manufacturer' values based on the custom mapping\n",
    "df['manufacturer'] = df.apply(\n",
    "    lambda row: infer_manufacturer_custom(row['base_model_normalized'], custom_mapping5) if pd.isnull(row['manufacturer']) else row['manufacturer'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop the temporary normalized base_model column if no longer needed\n",
    "df.drop('base_model_normalized', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49522ef2",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e0fb51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_model\n",
       "ALL                  52\n",
       "All                  46\n",
       "BUY                  40\n",
       "Janesville           31\n",
       "Flexible             30\n",
       "to                   24\n",
       "SPECIAL              19\n",
       "cars                 18\n",
       "Other                17\n",
       "Mobility             16\n",
       "Emergency            15\n",
       "any                  14\n",
       "Any                  13\n",
       "NEW                  13\n",
       "WOLF                 12\n",
       "THE                  10\n",
       "WATER                 9\n",
       "CHEC                  7\n",
       "Club                  7\n",
       "Town                  7\n",
       "Cars                  7\n",
       "todos                 7\n",
       "CLICK                 7\n",
       "wheelchair            7\n",
       "Western               6\n",
       "FABRIQUE              6\n",
       "ASK                   6\n",
       "am                    6\n",
       "2D8HN54159R611084     6\n",
       "Eagle                 6\n",
       "F8OO                  5\n",
       "John                  5\n",
       "FRHT                  5\n",
       "SUPER                 5\n",
       "BMX                   5\n",
       "Alpha                 5\n",
       "E350                  4\n",
       "2010-2017             4\n",
       "Jayco                 4\n",
       "AM                    4\n",
       "Van                   4\n",
       "hyandai               4\n",
       "GEHL                  4\n",
       "madza                 4\n",
       "Police                4\n",
       "merceds               4\n",
       "No                    4\n",
       "MANY                  4\n",
       "MBZ                   4\n",
       "Spartan               4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataframe for rows where 'manufacturer' is missing\n",
    "missing_manufacturer_df = df[df['manufacturer'].isnull()]\n",
    "\n",
    "# Count occurrences of each 'base_model' and get the top 10\n",
    "top_ten_base_models = missing_manufacturer_df['base_model'].value_counts().head(50)\n",
    "\n",
    "top_ten_base_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eea9a627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343615, 17)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6a1c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 343615\n",
      "region                404\n",
      "price               14630\n",
      "year                   28\n",
      "manufacturer           93\n",
      "fuel                    5\n",
      "odometer            91379\n",
      "title_status            6\n",
      "transmission            3\n",
      "type                   13\n",
      "state                  51\n",
      "posting_year            1\n",
      "posting_month           2\n",
      "posting_day            30\n",
      "posting_weekday         7\n",
      "posting_hour           24\n",
      "base_model           3453\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique values in each column of the DataFrame\n",
    "unique_values_count = df.nunique()\n",
    "\n",
    "# Print the count of unique values for each column\n",
    "print(unique_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f480e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     0\n",
      "region                 0\n",
      "price                  0\n",
      "year                   0\n",
      "manufacturer        1828\n",
      "fuel                1887\n",
      "odometer               0\n",
      "title_status        6513\n",
      "transmission        1385\n",
      "type               70898\n",
      "state                  0\n",
      "posting_year           0\n",
      "posting_month          0\n",
      "posting_day            0\n",
      "posting_weekday        0\n",
      "posting_hour           0\n",
      "base_model             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum of null values in each column\n",
    "null_values_sum = df.isnull().sum()\n",
    "\n",
    "# Display the sum of null values for each column\n",
    "print(null_values_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bb15655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   int64\n",
       "region              object\n",
       "price                int64\n",
       "year               float64\n",
       "manufacturer        object\n",
       "fuel                object\n",
       "odometer           float64\n",
       "title_status        object\n",
       "transmission        object\n",
       "type                object\n",
       "state               object\n",
       "posting_year         int32\n",
       "posting_month        int32\n",
       "posting_day          int32\n",
       "posting_weekday      int32\n",
       "posting_hour         int32\n",
       "base_model          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc19e04",
   "metadata": {},
   "source": [
    "## 'Type' Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae2cc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you're starting fresh with df2 being a copy of df\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de85cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoders = {}  # To store encoders for inverse transformation\n",
    "categorical_cols = ['region', 'manufacturer', 'fuel', 'title_status', 'transmission', 'state', 'base_model']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df2[col].dtype == 'object':  # Check if column is categorical\n",
    "        le = LabelEncoder()\n",
    "        df2[col] = df2[col].fillna('missing')  # Temporarily fill NaNs for encoding\n",
    "        df2[col] = le.fit_transform(df2[col])\n",
    "        encoders[col] = le  # Store encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e67f6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming NaN represents missing values in 'type'\n",
    "missing_type_indices = df2[df2['type'].isna()].index  # Identify missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8173b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily replace NaN with a placeholder, encode, then map back\n",
    "df2['type'] = df2['type'].fillna('missing')\n",
    "type_le = LabelEncoder()\n",
    "df2['type_encoded'] = type_le.fit_transform(df2['type'])  # Encode 'type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d8eff9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Exclude 'missing' type rows for training\n",
    "training_data = df2[df2['type'] != 'missing']\n",
    "\n",
    "X = training_data[categorical_cols]  # Features\n",
    "y = training_data['type_encoded']    # Encoded 'type'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0062f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(missing_type_indices) > 0:\n",
    "    X_missing = df2.loc[missing_type_indices, categorical_cols]\n",
    "    predicted_types_encoded = clf.predict(X_missing)\n",
    "    predicted_types = type_le.inverse_transform(predicted_types_encoded)  # Decode predictions\n",
    "    \n",
    "    # Update 'type' in df2 with predicted, decoded values\n",
    "    df2.loc[missing_type_indices, 'type'] = predicted_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd00ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse encoding for categorical columns\n",
    "for col, le in encoders.items():\n",
    "    df2[col] = le.inverse_transform(df2[col])\n",
    "\n",
    "# Ensure 'type' column is correctly handled\n",
    "df2.loc[df2['type'] == 'missing', 'type'] = np.nan  # Optional: Set 'missing' back to NaN if preferred\n",
    "\n",
    "# Clean up temporary encoding if needed\n",
    "df2.drop('type_encoded', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "adee9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the 'type' column in the original df with the updated values from df2\n",
    "df['type'] = df2['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12ecdd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\skyla\\Downloads\\vehiclescleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97515b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                    0\n",
      "region                0\n",
      "price                 0\n",
      "year                  0\n",
      "manufacturer       1828\n",
      "fuel               1887\n",
      "odometer              0\n",
      "title_status       6513\n",
      "transmission       1385\n",
      "type                  0\n",
      "state                 0\n",
      "posting_year          0\n",
      "posting_month         0\n",
      "posting_day           0\n",
      "posting_weekday       0\n",
      "posting_hour          0\n",
      "base_model            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum of null values in each column\n",
    "null_values_sum = df.isnull().sum()\n",
    "\n",
    "# Display the sum of null values for each column\n",
    "print(null_values_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aea414",
   "metadata": {},
   "source": [
    "## PySpark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9476216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/javajdk\"  # Use forward slashes or double backslashes\n",
    "os.environ[\"SPARK_HOME\"] = \"C:/spark\"   # Use forward slashes or double backslashes\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a88de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5143fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_401\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_401-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.401-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b596087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: C:/spark\n",
      "JAVA_HOME: C:/javajdk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"SPARK_HOME:\", os.environ.get(\"SPARK_HOME\"))\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49da073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed py4j version: 0.10.9\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# Attempt to get the distribution of py4j\n",
    "py4j_dist = pkg_resources.get_distribution(\"py4j\")\n",
    "\n",
    "if py4j_dist:\n",
    "    print(\"Installed py4j version:\", py4j_dist.version)\n",
    "else:\n",
    "    print(\"py4j is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b6f3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Vehicle Price Prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4b7caf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+------+------------+----+--------+------------+------------+------+-----+------------+-------------+-----------+---------------+------------+----------+\n",
      "|        id|region|price|  year|manufacturer|fuel|odometer|title_status|transmission|  type|state|posting_year|posting_month|posting_day|posting_weekday|posting_hour|base_model|\n",
      "+----------+------+-----+------+------------+----+--------+------------+------------+------+-----+------------+-------------+-----------+---------------+------------+----------+\n",
      "|7316814884|auburn|33590|2014.0|         gmc| gas| 57923.0|       clean|       other|pickup|   al|        2021|            5|          4|              1|          17|    sierra|\n",
      "|7316814758|auburn|22590|2010.0|   chevrolet| gas| 71229.0|       clean|       other|pickup|   al|        2021|            5|          4|              1|          17| silverado|\n",
      "|7316814989|auburn|39590|2020.0|   chevrolet| gas| 19160.0|       clean|       other|pickup|   al|        2021|            5|          4|              1|          17| silverado|\n",
      "|7316743432|auburn|30990|2017.0|      toyota| gas| 41124.0|       clean|       other|pickup|   al|        2021|            5|          4|              1|          15|    tundra|\n",
      "|7316356412|auburn|15000|2013.0|        ford| gas|128000.0|       clean|   automatic| truck|   al|        2021|            5|          3|              0|          19|     f-150|\n",
      "+----------+------+-----+------+------------+----+--------+------------+------------+------+-----+------------+-------------+-----------+---------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- odometer: double (nullable = true)\n",
      " |-- title_status: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- posting_year: integer (nullable = true)\n",
      " |-- posting_month: integer (nullable = true)\n",
      " |-- posting_day: integer (nullable = true)\n",
      " |-- posting_weekday: integer (nullable = true)\n",
      " |-- posting_hour: integer (nullable = true)\n",
      " |-- base_model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"file:///C:/Users/skyla/Downloads/vehiclescleaned.csv\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d867c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78d610a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 332428\n",
      "Number of columns: 17\n"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "num_rows = df.count()\n",
    "print(\"Number of rows:\", num_rows)\n",
    "\n",
    "# Column names (equivalent to shape[1] in Pandas)\n",
    "num_columns = len(df.columns)\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1ef78",
   "metadata": {},
   "source": [
    "## PySpark Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e48e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|price|            features|\n",
      "+------------------+-----+--------------------+\n",
      "|31589.999956437787|31590|[31590.0,2017.0,2...|\n",
      "|45590.000006029826|45590|[45590.0,2020.0,3...|\n",
      "|22989.999897354828|22990|[22990.0,2020.0,3...|\n",
      "| 25589.99991803942|25590|[25590.0,2018.0,3...|\n",
      "|20589.999991575594|20590|[20590.0,2013.0,7...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Import LinearRegression\n",
    "\n",
    "# Create a LinearRegression estimator\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Define a list of categorical columns\n",
    "categorical_columns = ['region', 'manufacturer', 'fuel', 'title_status', 'transmission', 'type', 'state', 'base_model']\n",
    "\n",
    "# Create a StringIndexer for each categorical column\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\") for column in categorical_columns]\n",
    "\n",
    "# Transform the training data using the indexers\n",
    "for indexer in indexers:\n",
    "    train_data = indexer.fit(train_data).transform(train_data)\n",
    "\n",
    "# Drop the original string columns from the training data\n",
    "train_data = train_data.drop(*categorical_columns)\n",
    "\n",
    "# Transform the test data using the indexers\n",
    "for indexer in indexers:\n",
    "    test_data = indexer.fit(test_data).transform(test_data)\n",
    "\n",
    "# Drop the original string columns from the test data\n",
    "test_data = test_data.drop(*categorical_columns)\n",
    "\n",
    "# Now, assemble the features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=train_data.columns[1:], outputCol=\"features\")\n",
    "\n",
    "# Transform the training data\n",
    "train_data = assembler.transform(train_data)\n",
    "\n",
    "# Transform the test data\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "# Now, train the linear regression model\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Once trained, you can use the model to make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Show some example predictions\n",
    "predictions.select(\"prediction\", \"price\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88b52534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 9.068182162941225e-05\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75a492",
   "metadata": {},
   "source": [
    "## PySpark Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f66ecff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Root Mean Squared Error (RMSE) on test data = 684.843792729947\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Define the decision tree regressor\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"price\", maxBins=3000)  # Set maxBins to a larger value\n",
    "\n",
    "# Train the decision tree model\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "dt_rmse = evaluator.evaluate(dt_predictions)\n",
    "\n",
    "print(\"Decision Tree Root Mean Squared Error (RMSE) on test data =\", dt_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444aff91",
   "metadata": {},
   "source": [
    "## PySpark Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2dae01c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Root Mean Squared Error (RMSE) on test data = 4003.354263095427\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Define the Random Forest regressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"price\", maxBins=3000)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(\"Random Forest Root Mean Squared Error (RMSE) on test data =\", rf_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72996397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054d764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234b4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
